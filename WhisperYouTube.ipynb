{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewMayneProjects/Whisper/blob/main/WhisperYouTube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're looking at this on GitHub and new to Python Notebooks or Colab, click the Google Colab badge above üëÜ\n",
        "\n",
        "#**Creating YouTube transcripts with OpenAI's Whisper model** \n",
        " \n",
        "*Colab beginner notes:*\n",
        "<br>\n",
        "1. These files are being loaded on a virtual machine in the cloud. Nothing is being downloaded to your computer (except for the transcript when you click to download it.) When you close this session the instance will be erased.\n",
        "<br>\n",
        "2. The run button is visible when you move your mouse close to the left edge of the code block. It looks kind of like this: ‚ñ∂Ô∏è ...but round...and white on black...so nothing like this. You'll know it when you see it.\n",
        "\n",
        "###**Note: For faster performance set your runtime to \"GPU\"**\n",
        "*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*\n",
        "\n",
        "\n",
        "**Step 1.** Follow the instructions in each block and select the options you want\n",
        "<br> \n",
        "**Step 2.** Get the url of the video you want to transcribe\n",
        "<br> \n",
        "**Step 3.** Refresh the folder on the left and download your transcript\n",
        "<br> \n",
        "**Step 4.** Go to your YouTube account and upload the transcript to the video it came from and use \"autosync.\"\n",
        "\n",
        "That's it!\n",
        "\n",
        "Have a question? Hit me up on Twitter:[ @AndrewMayne](https://twitter.com/andrewmayne)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What is this?**\n",
        "<br>\n",
        "This is a Python notebook that creates a transcript from a YouTube url using OpenAI's Whisper transcription model that you can then upload to YouTube using the autosync feature to create captions.\n",
        "<br>  \n",
        "**What is OpenAI's Whisper model?**\n",
        "<br>\n",
        "Whisper is an automatic speech recognition (ASR) neural net created by OpenAI that transcribes audio at close to human level.\n",
        "<br>\n",
        "<br>\n",
        "**Why use this?**\n",
        "<br> \n",
        "The quality of the OpenAI Whisper model is amazing (I am slightly biased, but seriously, check it out.) You can also use it to transcribe in other languages.\n",
        "<br> \n",
        "<br>\n",
        "**What do the different model sizes do?**\n",
        "<br>\n",
        "Each model size has an improvement in quality ‚Äì especially with different languages. I've found that for a YouTube video with clear speech, the base model works really well. If you see transcription errors, you can try a larger model.\n",
        "<br>\n",
        "<br> \n",
        "**Do I need timestamps?**\n",
        "<br> \n",
        "Nope. YouTube's autosync function will match the text to the spoken words and syncs up really well. All you need is each spoken sentence in a .txt file.\n",
        "<br> \n",
        "<br> \n",
        "**How do I do this?**\n",
        "<br> \n",
        "Just follow each step. If you've never used Colab of a Python notebook, don't panic. It's super easy and runs in the cloud.\n",
        "<br> \n",
        "<br> \n",
        "**Does this cost anything to use?**\n",
        "<br>\n",
        "Nope. You can use Colab for free and Whisper is an open source model. \n",
        "<br> \n",
        "<br> \n",
        "[Tips for creating a YouTube transcript file](https://support.google.com/youtube/answer/2734799?hl=en)\n",
        "<br> \n",
        "[Information on OpenAI's Whisper model](https://openai.com/blog/whisper/)\n",
        "<br> \n",
        "[OpenAI's Whisper GitHub page](https://github.com/openai/whisper)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvz5JoKjwKAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Click the start button in the upper left side of this block to load the necessary libraries\n",
        "\n",
        "You will need to run this every time you reload this notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip install youtube_dl\n",
        "!pip install git+https://github.com/openai/whisper.git \n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install librosa\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "import librosa\n",
        "import re\n",
        "import youtube_dl"
      ],
      "metadata": {
        "id": "j6svgIwL1a-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2. Select the model you want to use. \n",
        "\n",
        "Base works really well so it's the default. \n",
        "\n",
        "(For multilingual, remove \".en\" from the model name.)\n",
        "\n",
        "Click the run button after you've made your choice (or left it at default.)\n",
        "\"\"\"\n",
        "\n",
        "# model = whisper.load_model(\"tiny.en\")\n",
        "model = whisper.load_model(\"base.en\")\n",
        "# model = whisper.load_model(\"small.en\")\n",
        "# model = whisper.load_model(\"medium.en\")\n",
        "# model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "9oRA4UIe104O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3. Click the run button and input your YouTube URL in the box below then click enter.\n",
        "\n",
        "You can use this one to test: https://www.youtube.com/watch?v=CnT-Na1IeVI\n",
        "\n",
        "The video will be loaded and the audio extracted (this is usually the longest part of the process.)\n",
        "\n",
        "Your transcript will appear in the folder on the left (you may have to refresh the folder to see it.)\n",
        "\n",
        "You can download the file when it's completed and upload it on your video's detail page using \"autosync.\"\n",
        "\"\"\"\n",
        "\n",
        "# This will prompt you for a YouTube video URL\n",
        "url = input(\"Enter a YouTube video URL: \")\n",
        "\n",
        "# Create a youtube-dl options dictionary\n",
        "ydl_opts = {\n",
        "    # Specify the format as bestaudio/best\n",
        "    'format': 'bestaudio/best',\n",
        "    # Specify the post-processor as ffmpeg to extract audio and convert to mp3\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "    # Specify the output filename as the video title\n",
        "    'outtmpl': '%(title)s.%(ext)s',\n",
        "}\n",
        "\n",
        "# Download the video and extract the audio\n",
        "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([url])\n",
        "\n",
        "# Get the path of the file\n",
        "file_path = ydl.prepare_filename(ydl.extract_info(url, download=False))\n",
        "file_path = file_path.replace('.webm', '.mp3')\n",
        "file_path = file_path.replace('.m4a', '.mp3')\n",
        "\n",
        "# Get the duration\n",
        "duration = librosa.get_duration(filename=file_path)\n",
        "start = time.time()\n",
        "result = model.transcribe(file_path)\n",
        "end = time.time()\n",
        "seconds = end - start\n",
        "\n",
        "print(\"Video length:\", duration, \"seconds\")\n",
        "print(\"Transcription time:\", seconds)\n",
        "\n",
        "# Split result[\"text\"]  on !,? and . , but save the punctuation\n",
        "sentences = re.split(\"([!?.])\", result[\"text\"])\n",
        "\n",
        "# Join the punctuation back to the sentences\n",
        "sentences = [\"\".join(i) for i in zip(sentences[0::2], sentences[1::2])]\n",
        "text = \"\\n\\n\".join(sentences)\n",
        "for s in sentences:\n",
        "  print(s)\n",
        "\n",
        "# Save the file as .txt\n",
        "name = \"\".join(file_path) + \".txt\"\n",
        "with open(name, \"w\") as f:\n",
        "  f.write(text)\n",
        "\n",
        "print(\"\\n\\n\", \"-\"*100, \"\\n\\nYour transcript is here:\", name)"
      ],
      "metadata": {
        "id": "JmbHC2-S33Kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}